@Preamble{ " \newcommand{\noop}[1]{} " }

@Article{more_than_one_replication,
	abstract = {The problem of assessing whether experimental results can be replicated is becoming increasingly important in many areas of science. It is often assumed that assessing replication is straightforward: All one needs to do is repeat the study and see whether the results of the original and replication studies agree. This article shows that the statistical test for whether two studies obtain the same effect is smaller than the power of either study to detect an effect in the first place. Thus, unless the original study and the replication study have unusually high power (e.g., power of 98\%), a single replication study will not have adequate sensitivity to provide an unambiguous evaluation of replication.},
	author = {Larry V. Hedges and Jacob M. Schauer},
	doi = {10.3102/1076998619852953},
	journal = {Journal of Educational and Behavioral Statistics},
	title = {More than one replication study is needed for unambiguous tests of
replication},
	volume = {44},
	issue = {5},
	pages = {543-570},
	year = {2019}
}


@Article{history_of_rcts,
	abstract = {Background and purpose: Studies of education and learning that were described as experiments have been carried out in the USA by educational psychologists since about 1900. In this paper, we discuss the history of randomised trials in education in the USA in terms of five historical periods. In each period, the use of randomised trials was motivated by the research interests and conditions of the era. We have characterised these periods in terms of decades with sharp boundaries as a convenience.

	Sources of evidence and main arguments: Although some of the early studies used random allocation (and even random allocation of clusters such as schools), early researchers did not clearly understand the role of randomisation or clearly distinguish it from methods such as alternation. In 1940, E. F. Lindquist published an important book whose goal was to translate R. A. Fisher's ideas into language congenial to education researchers, but this had little impact on education research outside of psychology. There was a substantial increase in the number of randomised trials during the period from 1960 to 1980, as the US government enacted and evaluated a variety of social programmes. This was followed by a dramatic decrease during the period from 1980 to 2000, amid debates about the relevance of randomised trials in education research. The creation of the US Institute of Education Sciences in 2002 provided major financial and administrative support for randomised trials, which has led to a large number of trials being conducted since that time.

	Conclusions: These developments suggest that there is a promising future for randomised trials in the USA. American education scientists must remain committed to explaining why evidence from randomised field trials has an indispensable role to play in making wise decisions about education policy and advancing our capacity to improve education for a productive workforce and a successful society.},
	author = {Larry V. Hedges and Jacob M. Schauer},
	doi = {10.1080/00131881.2018.1493350},
	journal = {Educational Research},
	number = {3},
	pages = {265-275},
	publisher = {Routledge},
	title = {Randomised trials in education in the USA},
	volume = {60},
	year = {2018}
}

@Article{replication_meta-analytic_perspectives,
	abstract = {Formal empirical assessments of replication have recently become more prominent in several areas of science, including psychology. These assessments have used different statistical approaches to determine if a finding has been replicated. The purpose of this article is to provide several alternative conceptual frameworks that lead to different statistical analyses to test hypotheses about replication. All of these analyses are based on statistical methods used in meta-analysis. The differences among the methods described involve whether the burden of proof is placed on replication or nonreplication, whether replication is exact or allows for a small amount of "negligible heterogeneity," and whether the studies observed are assumed to be fixed (constituting the entire body of relevant evidence) or are a sample from a universe of possibly relevant studies. The statistical power of each of these tests is computed and shown to be low in many cases, raising issues of the interpretability of tests for replication.},
	author = {Larry V. Hedges and Jacob M. Schauer},
	doi = {10.1037/met0000189},
	journal = {Psychological Methods},
	title = {Statistical analyses for studying replication: Meta-analytic perspectives},
	volume = {24},
	issue = {5},
	pages = {557-570},
	year = {2019}
}


@Article{replication-consistency-of-effects,
	abstract = {In this rejoinder, we discuss Mathur and VanderWeele's response to our article, "Statistical Analyses for
	Studying Replication: Meta-Analytic Perspectives," which appears in this current issue. We attempt to
	clarify a point of confusion regarding the inclusion of an original study in an analysis of replication, and
	the potential impact of publication bias. We then discuss the methods used by Mathur and VanderWeele
	to conduct an alternative analysis of the Gambler's Fallacy example from our article. We highlight that
	there are some potential statistical and conceptual differences to their approach compared to what we
	propose in our article},
	author = {Larry V. Hedges and Jacob M. Schauer},
	journal = {Psychological Methods},
	title = {Consistency of effects is important in replication},
	volume = {24},
	issue = {5},
	pages = {576-577},
	doi = {10.1037/met0000237},
	year = {2019}
}



@Article{assessing_heterogeneity,
	abstract = {In this study, we reanalyze recent empirical research on replication from a meta-analytic perspective. We argue that there are different ways to define 'replication failure,' and that analyses can focus on exploring variation among replication studies or assess whether their results contradict the findings of the original study. We apply this framework to a set of psychological findings that have been replicated and assess the sensitivity of these analyses. We find that tests for replication that involve only a single replication study are almost always severely underpowered. Among the 40 findings for which ensembles of multisite direct replications were conducted, we find that between 11 and 17 (28\% to 43\%) ensembles produced heterogeneous effects, depending on how replication is defined. This heterogeneity could not be completely explained by moderators documented by replication research programs. We also find that these ensembles were not always well-powered to detect potentially meaningful values of heterogeneity. Finally, we identify several discrepancies between the results of original studies and the distribution of effects found by multisite replications but note that these analyses also have low power. We conclude by arguing that efforts to assess replication would benefit from further methodological work on designing replication studies to ensure analyses are sufficiently sensitive.},
	author = {Jacob M. Schauer and Larry V. Hedges},
	journal = {Psychological Bulletin},
	title = {Assessing heterogeneity and power in replications of psychological experiments},
	doi = {10.1037/bul0000232},
	volume = {146},
	issue = {8},
	pages = {701-719},
	year = {2020}
}


@Article{reconsidering_metrics,
	abstract = {Recent empirical evaluations of replication in psychology have reported startlingly few successful replication attempts. At the same time, they have noted that the proper way to analyze replication studies is far from a settled matter and have thus analyzed their data in several different ways. This presents two challenges to interpreting the results of these programs. First, different analysis methods assess different operational definitions of replication. Second, the properties of these methods are not necessarily common knowledge; it is possible for a successful replication to be deemed a failure by nearly all of the metrics used, and it is not always immediately clear how likely such errors are to occur. In this article, we describe the methods commonly used in replication research and how they imply specific operational definitions of replication. We then compute the probability of false failure (i.e., a successful replication is concluded to have failed) and false success determinations. These are shown to be high (often over 50\%) and in many cases uncontrolled. We then demonstrate that errors are probable in the data to which they have been applied in the literature. We show that the probability that some conclusions in the literature about replication are incorrect can be as high as 75-80\%.},
	author = {Jacob M. Schauer and Larry V. Hedges},
	journal = {Psychological Methods},
	title = {Reconsidering statistical methods for assessing replication},
	doi = {10.1037/met0000302},
  journal = {Psychological Methods},
  volume = {26},
  issue = {1},
  pages = {127--139},
  doi = {10.1037/met0000302},
  year = {2021}
}


@Article{microsuppression,
	abstract = {States often turn to a data masking procedure called microsuppression in order to reduce the risk of disclosing student records when sharing data with external researchers. This process removes records deemed to have high risk for disclosure should they be released. However, this process can lead to analyses that differ from those conducted on the complete (unmasked) data, especially if the records that are released reflect different types of students than those that are suppressed. This paper assesses the extent to which microsuppression can bias parameter estimates, and finds that while marginal test score means tend to be preserved in the masked data, conditional means for subgroups can exhibit bias as large as 0.3 standard deviations.},
	author = {Jacob M. Schauer and Arend M. Kuyper and Eric C. Hedberg and Larry V. Hedges},
	journal = {Journal of Research on Educational Effectiveness},
	title = {The effects of microsuppression on state education data quality},
	doi = {10.1080/19345747.2020.1814465},
	volume = {13},
	issue = {4},
	pages = {794-815},
	year = {2020}
}

@Article{aggregate_metrics,
 abstract = {Several programs of research have sought to assess the replicability of scientific findings in different fields, including economics and psychology. These programs attempt to replicate several findings and use the results to say something about large-scale patterns of replicability in a field. However, little work has been done to understand the analytic methods used to do this, including what they are assessing and what their statistical properties are. This article examines several methods that have been used to study patterns of replicability in the social sciences. We describe in concrete terms how each method operationalizes the idea of "replication" and examine various statistical properties, including bias, precision, and statistical power. We find that some analytic methods rely on an operational definition of replication that can be misleading. Other methods involve more sound definitions of replication, but most of these have limitations such as large bias and uncertainty or low power. The findings suggest that we should use caution interpreting the results of such analyses and that work on more accurate methods may be useful to future replication research efforts.},
 author = {Jacob M. Schauer and Kaitlyn G. Fitzgerald and Sarah Peko-Spicer and Mena C. R. Whalen and Rrita Zejnullahi and Larry V. Hedges},
 journal = {Annals of Applied Statistics},
 title = {An evaluation of statistical methods for aggregate patterns of replication failure},
 journal = {Annals of Applied Statistics},
 volume = {15},
 issue = {1},
 pages = {208 - 229},
 doi = {10.1214/20-AOAS1387},
 year = {2021}
}


@Article{missing_data_diagnostics,
 author = {Jacob M. Schauer and Karina Dìaz and Jihyun Lee and Therese D. Pigott},
 journal = {Alcohol and Alcoholism},
 title = {Exploratory analyses for missing data in meta-analyses},
 volume = {57},
 issue = {1},
 pages = {35 - 47},
 doi = {10.1093/alcalc/agaa144},
 year = "2021",
 abstract = "OBJECTIVES: In this tutorial, we examine methods for exploring missingness in a dataset in ways that can help identify the sources and extent of missingness, as well as clarify gaps in evidence.

METHODS: Using raw data from a meta-analysis of substance abuse interventions, we demonstrate the use of exploratory missingness analysis (EMA) including techniques for numerical summaries and visual displays of missing data.

RESULTS: These techniques examine the patterns of missing covariates in meta-analysis data and the relationships among variables with missing data and observed variables including the effect size. The case study shows complex relationships among missingness and other potential covariates in meta-regression, highlighting gaps in the evidence base.

CONCLUSION: Meta-analysts could often benefit by employing some form of EMA as they encounter missing data."
}

@Article{design_of_replication_studies,
 author = {Larry V. Hedges and Jacob M. Schauer},
 journal = {Journal of the Royal Statistical Society: Series A},
 title = {The design of replication studies},
 abstract = "Empirical evaluations of replication have become increasingly common, ranging from systematic attempts of one-off replications (e.g., Open Science Collaboration, 2015) to the Many Labs approach, where multiple labs independently run the same experiment (Klein et al., 2014). Designing such programs has largely contended with difficult issues about which experimental components are necessary for a set of studies to be considered replicates. However, another important consideration is that replicate studies be designed to support sufficiently sensitive analyses. For instance, if hypothesis tests are to be conducted about replication, studies should be designed to ensure these tests are well-powered; if not, it can be difficult to determine conclusively if replication attempts succeeded or failed. This paper describes methods for designing ensembles of replication studies to ensure that they are both adequately sensitive and cost-efficient. It describes two potential analyses of replication studies–hypothesis tests and variance component estimation–and approaches to obtaining optimal designs for them. Using these results, it assesses the sensitivity and optimality of the Many Labs design and finds that while it may have been sufficiently powered to detect some larger differences between studies, other designs would have been less costly or more sensitive (or in some cases, both).",
 volume = {184},
 pages = {868 - 886},
 doi = {10.1111/rssa.12688},
 year = "2021"
}

@article{failure_rates,
 abstract = {A prominent approach to studying the replication crisis has been to conduct replications of several different scientific findings as part of the same research effort. The reported proportion of findings that these programs determined failed to replicate have become important statistics in the replication crisis. However, these "failure rates" are based on decisions about whether individual studies replicated, which are themselves subject to some statistical uncertainty. In this article, we examine how that uncertainty impacts the accuracy of reported failure rates and find that the reported failure rates can be substantially biased and highly variable. Indeed, very high or very low failure rates could arise from chance alone.},
 author = {Jacob M. Schauer},
 journal = {Multivariate Behavioral Research},
 title = {On the accuracy of replication failure rates},
 year = {in press}
}

@article{cca_aca_meta_regression,
 abstract = {},
 author = {Jacob M. Schauer and Karina Dìaz and Jihyun Lee and Therese D. Pigott},
 journal = {Research Synthesis Methods},
 title = {On the bias of complete- and shifting-case meta-regressions with missing covariates},
 year = {in press},
 url = {https://onlinelibrary.wiley.com/doi/10.1002/jrsm.1558},
 doi = {doi/10.1002/jrsm.1558}
}

