@article{more_than_one_replication,
 abstract = {The problem of assessing whether experimental results can be replicated is becoming increasingly important in many areas of science. It is often assumed that assessing replication is straightforward: All one needs to do is repeat the study and see whether the results of the original and replication studies agree. This article shows that the statistical test for whether two studies obtain the same effect is smaller than the power of either study to detect an effect in the first place. Thus, unless the original study and the replication study have unusually high power (e.g., power of 98%), a single replication study will not have adequate sensitivity to provide an unambiguous evaluation of replication.},
 author = {Larry V. Hedges and Jacob M. Schauer},
 doi = {10.3102/1076998619852953},
 journal = {Journal of Educational and Behavioral Statistics},
 title = {More than one replication study is needed for unambiguous tests of
replication},
 year = {in press}
}


@article{history_of_rcts,
 abstract = {Background and purpose: Studies of education and learning that were described as experiments have been carried out in the USA by educational psychologists since about 1900. In this paper, we discuss the history of randomised trials in education in the USA in terms of five historical periods. In each period, the use of randomised trials was motivated by the research interests and conditions of the era. We have characterised these periods in terms of decades with sharp boundaries as a convenience.

Sources of evidence and main arguments: Although some of the early studies used random allocation (and even random allocation of clusters such as schools), early researchers did not clearly understand the role of randomisation or clearly distinguish it from methods such as alternation. In 1940, E. F. Lindquist published an important book whose goal was to translate R. A. Fisherâs ideas into language congenial to education researchers, but this had little impact on education research outside of psychology. There was a substantial increase in the number of randomised trials during the period from 1960 to 1980, as the US government enacted and evaluated a variety of social programmes. This was followed by a dramatic decrease during the period from 1980 to 2000, amid debates about the relevance of randomised trials in education research. The creation of the US Institute of Education Sciences in 2002 provided major financial and administrative support for randomised trials, which has led to a large number of trials being conducted since that time.

Conclusions: These developments suggest that there is a promising future for randomised trials in the USA. American education scientists must remain committed to explaining why evidence from randomised field trials has an indispensable role to play in making wise decisions about education policy and advancing our capacity to improve education for a productive workforce and a successful society.},
 author = {Larry V. Hedges and Jacob M. Schauer},
 doi = {10.1080/00131881.2018.1493350},
 eprint = { 
https://doi.org/10.1080/00131881.2018.1493350

},
 journal = {Educational Research},
 number = {3},
 pages = {265-275},
 publisher = {Routledge},
 title = {Randomised trials in education in the USA},
 url = { 
https://doi.org/10.1080/00131881.2018.1493350

},
 volume = {60},
 year = {2018}
}

@article{replication_meta-analytic_perspectives,
 abstract = {Formal empirical assessments of replication have recently become more prominent in several areas of science, including psychology. These assessments have used different statistical approaches to determine if a finding has been replicated. The purpose of this article is to provide several alternative conceptual frameworks that lead to different statistical analyses to test hypotheses about replication. All of these analyses are based on statistical methods used in meta-analysis. The differences among the methods described involve whether the burden of proof is placed on replication or nonreplication, whether replication is exact or allows for a small amount of ânegligible heterogeneity,â and whether the studies observed are assumed to be fixed (constituting the entire body of relevant evidence) or are a sample from a universe of possibly relevant studies. The statistical power of each of these tests is computed and shown to be low in many cases, raising issues of the interpretability of tests for replication.},
 author = {Larry V. Hedges and Jacob M. Schauer},
 doi = {10.1037/met0000189},
 journal = {Psychological Methods},
 title = {Statistical analyses for studying replication: Meta-analytic perspectives},
 year = {2019}
}


@article{replication-consistency-of-effects,
abstract = {In this rejoinder, we discuss Mathur and VanderWeele’s response to our article, “Statistical Analyses for
Studying Replication: Meta-Analytic Perspectives,” which appears in this current issue. We attempt to
clarify a point of confusion regarding the inclusion of an original study in an analysis of replication, and
the potential impact of publication bias. We then discuss the methods used by Mathur and VanderWeele
to conduct an alternative analysis of the Gambler’s Fallacy example from our article. We highlight that
there are some potential statistical and conceptual differences to their approach compared to what we
propose in our article},
author = {Larry V. Hedges and Jacob M. Schauer},
journal = {Psychological Methods},
title = {Consistency of effects is important in replication},
year = {in press}
}